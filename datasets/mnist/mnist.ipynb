{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/GLI/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# This notebook is adapted from https://github.com/graphdeeplearning/benchmarking-gnns/blob/master/data/superpixels/prepare_superpixels_CIFAR.ipynb\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already downloaded\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('superpixels.zip'):\n",
    "    print('downloading..')\n",
    "    from gli.utils import _download\n",
    "    _download(\"https://www.dropbox.com/s/y2qwa77a0fxem47/superpixels.zip?dl=1\", \"superpixels.zip\", verbose=True)\n",
    "    !mkdir -p data\n",
    "    !unzip superpixels.zip -d ./data/\n",
    "    # !tar -xvf superpixels.zip -C ../\n",
    "else:\n",
    "    print('File already downloaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/graphdeeplearning/benchmarking-gnns/blob/master/data/superpixels.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "import time\n",
    "\n",
    "import csv\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sigma(dists, kth=8):\n",
    "    # Compute sigma and reshape\n",
    "    try:\n",
    "        # Get k-nearest neighbors for each node\n",
    "        knns = np.partition(dists, kth, axis=-1)[:, kth::-1]\n",
    "        sigma = knns.sum(axis=1).reshape((knns.shape[0], 1))/kth\n",
    "    except ValueError:     # handling for graphs with num_nodes less than kth\n",
    "        num_nodes = dists.shape[0]\n",
    "        # this sigma value is irrelevant since not used for final compute_edge_list\n",
    "        sigma = np.array([1]*num_nodes).reshape(num_nodes,1)\n",
    "        \n",
    "    return sigma + 1e-8 # adding epsilon to avoid zero value of sigma\n",
    "\n",
    "\n",
    "def compute_adjacency_matrix_images(coord, feat, use_feat=True, kth=8):\n",
    "    coord = coord.reshape(-1, 2)\n",
    "    # Compute coordinate distance\n",
    "    c_dist = cdist(coord, coord)\n",
    "    \n",
    "    if use_feat:\n",
    "        # Compute feature distance\n",
    "        f_dist = cdist(feat, feat)\n",
    "        # Compute adjacency\n",
    "        A = np.exp(- (c_dist/sigma(c_dist))**2 - (f_dist/sigma(f_dist))**2 )\n",
    "    else:\n",
    "        A = np.exp(- (c_dist/sigma(c_dist))**2)\n",
    "        \n",
    "    # Convert to symmetric matrix\n",
    "    A = 0.5 * (A + A.T)\n",
    "    A[np.diag_indices_from(A)] = 0\n",
    "    return A        \n",
    "\n",
    "\n",
    "def compute_edges_list(A, kth=8+1):\n",
    "    # Get k-similar neighbor indices for each node\n",
    "\n",
    "    num_nodes = A.shape[0]\n",
    "    new_kth = num_nodes - kth\n",
    "    \n",
    "    if num_nodes > 9:\n",
    "        knns = np.argpartition(A, new_kth-1, axis=-1)[:, new_kth:-1]\n",
    "        knn_values = np.partition(A, new_kth-1, axis=-1)[:, new_kth:-1] # NEW\n",
    "    else:\n",
    "        # handling for graphs with less than kth nodes\n",
    "        # in such cases, the resulting graph will be fully connected\n",
    "        knns = np.tile(np.arange(num_nodes), num_nodes).reshape(num_nodes, num_nodes)\n",
    "        knn_values = A # NEW\n",
    "        \n",
    "        # removing self loop\n",
    "        if num_nodes != 1:\n",
    "            knn_values = A[knns != np.arange(num_nodes)[:,None]].reshape(num_nodes,-1) # NEW\n",
    "            knns = knns[knns != np.arange(num_nodes)[:,None]].reshape(num_nodes,-1)\n",
    "    return knns, knn_values # NEW\n",
    "\n",
    "\n",
    "class SuperPixDGL(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_dir,\n",
    "                 dataset,\n",
    "                 split,\n",
    "                 use_mean_px=True,\n",
    "                 use_coord=True):\n",
    "\n",
    "        self.split = split\n",
    "        \n",
    "        self.graph_lists = []\n",
    "        \n",
    "        if dataset == 'MNIST':\n",
    "            self.img_size = 28\n",
    "            with open(os.path.join(data_dir, 'mnist_75sp_%s.pkl' % split), 'rb') as f:\n",
    "                self.labels, self.sp_data = pickle.load(f)\n",
    "                self.graph_labels = torch.LongTensor(self.labels)\n",
    "        elif dataset == 'CIFAR10':\n",
    "            self.img_size = 32\n",
    "            with open(os.path.join(data_dir, 'cifar10_150sp_%s.pkl' % split), 'rb') as f:\n",
    "                self.labels, self.sp_data = pickle.load(f)\n",
    "                self.graph_labels = torch.LongTensor(self.labels)\n",
    "                \n",
    "        self.use_mean_px = use_mean_px\n",
    "        self.use_coord = use_coord\n",
    "        self.n_samples = len(self.labels)\n",
    "        \n",
    "        self._prepare()\n",
    "    \n",
    "    def _prepare(self):\n",
    "        print(\"preparing %d graphs for the %s set...\" % (self.n_samples, self.split.upper()))\n",
    "        self.Adj_matrices, self.node_features, self.edges_lists, self.edge_features = [], [], [], []\n",
    "        for index, sample in enumerate(self.sp_data):\n",
    "            mean_px, coord = sample[:2]\n",
    "            \n",
    "            try:\n",
    "                coord = coord / self.img_size\n",
    "            except AttributeError:\n",
    "                VOC_has_variable_image_sizes = True\n",
    "                \n",
    "            if self.use_mean_px:\n",
    "                A = compute_adjacency_matrix_images(coord, mean_px) # using super-pixel locations + features\n",
    "            else:\n",
    "                A = compute_adjacency_matrix_images(coord, mean_px, False) # using only super-pixel locations\n",
    "            edges_list, edge_values_list = compute_edges_list(A) # NEW\n",
    "\n",
    "            N_nodes = A.shape[0]\n",
    "            \n",
    "            mean_px = mean_px.reshape(N_nodes, -1)\n",
    "            coord = coord.reshape(N_nodes, 2)\n",
    "            x = np.concatenate((mean_px, coord), axis=1)\n",
    "\n",
    "            edge_values_list = edge_values_list.reshape(-1) # NEW # TO DOUBLE-CHECK !\n",
    "            \n",
    "            self.node_features.append(x)\n",
    "            self.edge_features.append(edge_values_list) # NEW\n",
    "            self.Adj_matrices.append(A)\n",
    "            self.edges_lists.append(edges_list)\n",
    "        \n",
    "        for index in range(len(self.sp_data)):\n",
    "            g = dgl.DGLGraph()\n",
    "            g.add_nodes(self.node_features[index].shape[0])\n",
    "            g.ndata['feat'] = torch.Tensor(self.node_features[index]).half() \n",
    "\n",
    "            for src, dsts in enumerate(self.edges_lists[index]):\n",
    "                # handling for 1 node where the self loop would be the only edge\n",
    "                # since, VOC Superpixels has few samples (5 samples) with only 1 node\n",
    "                if self.node_features[index].shape[0] == 1:\n",
    "                    g.add_edges(src, dsts)\n",
    "                else:\n",
    "                    g.add_edges(src, dsts[dsts!=src])\n",
    "            \n",
    "            # adding edge features for Residual Gated ConvNet\n",
    "            edge_feat_dim = g.ndata['feat'].shape[1] # dim same as node feature dim\n",
    "            #g.edata['feat'] = torch.ones(g.number_of_edges(), edge_feat_dim).half() \n",
    "            g.edata['feat'] = torch.Tensor(self.edge_features[index]).unsqueeze(1).half()  # NEW \n",
    "\n",
    "            self.graph_lists.append(g)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of graphs in the dataset.\"\"\"\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "            Get the idx^th sample.\n",
    "            Parameters\n",
    "            ---------\n",
    "            idx : int\n",
    "                The sample index.\n",
    "            Returns\n",
    "            -------\n",
    "            (dgl.DGLGraph, int)\n",
    "                DGLGraph with node feature stored in `feat` field\n",
    "                And its label.\n",
    "        \"\"\"\n",
    "        return self.graph_lists[idx], self.graph_labels[idx]\n",
    "\n",
    "\n",
    "class DGLFormDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        DGLFormDataset wrapping graph list and label list as per pytorch Dataset.\n",
    "        *lists (list): lists of 'graphs' and 'labels' with same len().\n",
    "    \"\"\"\n",
    "    def __init__(self, *lists):\n",
    "        assert all(len(lists[0]) == len(li) for li in lists)\n",
    "        self.lists = lists\n",
    "        self.graph_lists = lists[0]\n",
    "        self.graph_labels = lists[1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(li[index] for li in self.lists)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lists[0])\n",
    "    \n",
    "    \n",
    "class SuperPixDatasetDGL(torch.utils.data.Dataset):\n",
    "    def __init__(self, name, num_val=5000):\n",
    "        \"\"\"\n",
    "            Takes input standard image dataset name (MNIST/CIFAR10) \n",
    "            and returns the superpixels graph.\n",
    "            \n",
    "            This class uses results from the above SuperPix class.\n",
    "            which contains the steps for the generation of the Superpixels\n",
    "            graph from a superpixel .pkl file that has been given by\n",
    "            https://github.com/bknyaz/graph_attention_pool\n",
    "            \n",
    "            Please refer the SuperPix class for details.\n",
    "        \"\"\"\n",
    "        t_data = time.time()\n",
    "        self.name = name\n",
    "\n",
    "        use_mean_px = True # using super-pixel locations + features\n",
    "        use_mean_px = False # using only super-pixel locations\n",
    "        if use_mean_px:\n",
    "            print('Adj matrix defined from super-pixel locations + features')\n",
    "        else:\n",
    "            print('Adj matrix defined from super-pixel locations (only)')\n",
    "        use_coord = True\n",
    "        self.test = SuperPixDGL(\"./data/superpixels\", dataset=self.name, split='test', \n",
    "                            use_mean_px=use_mean_px, \n",
    "                            use_coord=use_coord)\n",
    "\n",
    "        self.train_ = SuperPixDGL(\"./data/superpixels\", dataset=self.name, split='train', \n",
    "                             use_mean_px=use_mean_px, \n",
    "                             use_coord=use_coord)\n",
    "\n",
    "        _val_graphs, _val_labels = self.train_[:num_val]\n",
    "        _train_graphs, _train_labels = self.train_[num_val:]\n",
    "\n",
    "        self.val = DGLFormDataset(_val_graphs, _val_labels)\n",
    "        self.train = DGLFormDataset(_train_graphs, _train_labels)\n",
    "\n",
    "        print(\"[I] Data load time: {:.4f}s\".format(time.time()-t_data))\n",
    "        \n",
    "\n",
    "\n",
    "def self_loop(g):\n",
    "    \"\"\"\n",
    "        Utility function only, to be used only when necessary as per user self_loop flag\n",
    "        : Overwriting the function dgl.transform.add_self_loop() to not miss ndata['feat'] and edata['feat']\n",
    "        \n",
    "        \n",
    "        This function is called inside a function in SuperPixDataset class.\n",
    "    \"\"\"\n",
    "    new_g = dgl.DGLGraph()\n",
    "    new_g.add_nodes(g.number_of_nodes())\n",
    "    new_g.ndata['feat'] = g.ndata['feat']\n",
    "    \n",
    "    src, dst = g.all_edges(order=\"eid\")\n",
    "    src = dgl.backend.zerocopy_to_numpy(src)\n",
    "    dst = dgl.backend.zerocopy_to_numpy(dst)\n",
    "    non_self_edges_idx = src != dst\n",
    "    nodes = np.arange(g.number_of_nodes())\n",
    "    new_g.add_edges(src[non_self_edges_idx], dst[non_self_edges_idx])\n",
    "    new_g.add_edges(nodes, nodes)\n",
    "    \n",
    "    # This new edata is not used since this function gets called only for GCN, GAT\n",
    "    # However, we need this for the generic requirement of ndata and edata\n",
    "    new_g.edata['feat'] = torch.zeros(new_g.number_of_edges())\n",
    "    return new_g\n",
    "\n",
    "    \n",
    "\n",
    "class SuperPixDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"\n",
    "            Loading Superpixels datasets\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        print(\"[I] Loading dataset %s...\" % (name))\n",
    "        self.name = name\n",
    "        data_dir = 'data/superpixels/'\n",
    "        with open(data_dir+name+'.pkl',\"rb\") as f:\n",
    "            f = pickle.load(f)\n",
    "            self.train = f[0]\n",
    "            self.val = f[1]\n",
    "            self.test = f[2]\n",
    "        print('train, test, val sizes :',len(self.train),len(self.test),len(self.val))\n",
    "        print(\"[I] Finished loading.\")\n",
    "        print(\"[I] Data load time: {:.4f}s\".format(time.time()-start))\n",
    "\n",
    "\n",
    "    # form a mini batch from a given list of samples = [(graph, label) pairs]\n",
    "    def collate(self, samples):\n",
    "        # The input samples is a list of pairs (graph, label).\n",
    "        graphs, labels = map(list, zip(*samples))\n",
    "        labels = torch.tensor(np.array(labels))\n",
    "        #tab_sizes_n = [ graphs[i].number_of_nodes() for i in range(len(graphs))]\n",
    "        #tab_snorm_n = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_n ]\n",
    "        #snorm_n = torch.cat(tab_snorm_n).sqrt()  \n",
    "        #tab_sizes_e = [ graphs[i].number_of_edges() for i in range(len(graphs))]\n",
    "        #tab_snorm_e = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_e ]\n",
    "        #snorm_e = torch.cat(tab_snorm_e).sqrt()\n",
    "        for idx, graph in enumerate(graphs):\n",
    "            graphs[idx].ndata['feat'] = graph.ndata['feat'].float()\n",
    "            graphs[idx].edata['feat'] = graph.edata['feat'].float()\n",
    "        batched_graph = dgl.batch(graphs)\n",
    "        \n",
    "        return batched_graph, labels\n",
    "    \n",
    "    \n",
    "    # prepare dense tensors for GNNs using them; such as RingGNN, 3WLGNN\n",
    "    def collate_dense_gnn(self, samples):\n",
    "        # The input samples is a list of pairs (graph, label).\n",
    "        graphs, labels = map(list, zip(*samples))\n",
    "        labels = torch.tensor(np.array(labels))\n",
    "        #tab_sizes_n = [ graphs[i].number_of_nodes() for i in range(len(graphs))]\n",
    "        #tab_snorm_n = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_n ]\n",
    "        #snorm_n = tab_snorm_n[0][0].sqrt()  \n",
    "        \n",
    "        #batched_graph = dgl.batch(graphs)\n",
    "    \n",
    "        g = graphs[0]\n",
    "        adj = self._sym_normalize_adj(g.adjacency_matrix().to_dense())        \n",
    "        \"\"\"\n",
    "            Adapted from https://github.com/leichen2018/Ring-GNN/\n",
    "            Assigning node and edge feats::\n",
    "            we have the adjacency matrix in R^{n x n}, the node features in R^{d_n} and edge features R^{d_e}.\n",
    "            Then we build a zero-initialized tensor, say T, in R^{(1 + d_n + d_e) x n x n}. T[0, :, :] is the adjacency matrix.\n",
    "            The diagonal T[1:1+d_n, i, i], i = 0 to n-1, store the node feature of node i. \n",
    "            The off diagonal T[1+d_n:, i, j] store edge features of edge(i, j).\n",
    "        \"\"\"\n",
    "\n",
    "        zero_adj = torch.zeros_like(adj)\n",
    "        \n",
    "        in_dim = g.ndata['feat'].shape[1]\n",
    "        \n",
    "        # use node feats to prepare adj\n",
    "        adj_node_feat = torch.stack([zero_adj for j in range(in_dim)])\n",
    "        adj_node_feat = torch.cat([adj.unsqueeze(0), adj_node_feat], dim=0)\n",
    "        \n",
    "        for node, node_feat in enumerate(g.ndata['feat']):\n",
    "            adj_node_feat[1:, node, node] = node_feat\n",
    "\n",
    "        x_node_feat = adj_node_feat.unsqueeze(0)\n",
    "        \n",
    "        return x_node_feat, labels\n",
    "    \n",
    "    def _sym_normalize_adj(self, adj):\n",
    "        deg = torch.sum(adj, dim = 0)#.squeeze()\n",
    "        deg_inv = torch.where(deg>0, 1./torch.sqrt(deg), torch.zeros(deg.size()))\n",
    "        deg_inv = torch.diag(deg_inv)\n",
    "        return torch.mm(deg_inv, torch.mm(adj, deg_inv))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _add_self_loops(self):\n",
    "        \n",
    "        # function for adding self loops\n",
    "        # this function will be called only if self_loop flag is True\n",
    "            \n",
    "        self.train.graph_lists = [self_loop(g) for g in self.train.graph_lists]\n",
    "        self.val.graph_lists = [self_loop(g) for g in self.val.graph_lists]\n",
    "        self.test.graph_lists = [self_loop(g) for g in self.test.graph_lists]\n",
    "        \n",
    "        self.train = DGLFormDataset(self.train.graph_lists, self.train.graph_labels)\n",
    "        self.val = DGLFormDataset(self.val.graph_lists, self.val.graph_labels)\n",
    "        self.test = DGLFormDataset(self.test.graph_lists, self.test.graph_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adj matrix defined from super-pixel locations (only)\n",
      "preparing 10000 graphs for the TEST set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/GLI/lib/python3.6/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing 60000 graphs for the TRAIN set...\n",
      "[I] Data load time: 1632.9907s\n"
     ]
    }
   ],
   "source": [
    "dataset = SuperPixDatasetDGL('MNIST') # 54s\n",
    "trainset, valset, testset = dataset.train, dataset.val, dataset.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SuperPixDatasetDGL object at 0x7fe9b6dd93c8>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, valset, testset = dataset.train, dataset.val, dataset.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1504]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.train[0][0].edges[0][0]['feat'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx=np.arange(0,55000,1)\n",
    "valid_idx=np.arange(55000,60000,1)\n",
    "test_idx=np.arange(60000,70000,1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save all dense arrays to mnist_task.npz, including ['train', 'val', 'test']\n"
     ]
    }
   ],
   "source": [
    "task_data = {\n",
    "    \"train\": train_idx,\n",
    "    \"val\": valid_idx,\n",
    "    \"test\": test_idx\n",
    "}\n",
    "\n",
    "from gli.utils import save_data\n",
    "save_data(\"mnist_task\", **task_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1504 ],\n",
       "       [0.2255 ],\n",
       "       [0.2622 ],\n",
       "       [0.272  ],\n",
       "       [0.2969 ],\n",
       "       [0.442  ],\n",
       "       [0.595  ],\n",
       "       [0.4353 ],\n",
       "       [0.3813 ],\n",
       "       [0.3125 ],\n",
       "       [0.5063 ],\n",
       "       [0.3474 ],\n",
       "       [0.09814],\n",
       "       [0.2854 ],\n",
       "       [0.5244 ],\n",
       "       [0.5957 ],\n",
       "       [0.15   ],\n",
       "       [0.2357 ],\n",
       "       [0.2134 ],\n",
       "       [0.2927 ],\n",
       "       [0.346  ],\n",
       "       [0.447  ],\n",
       "       [0.5337 ],\n",
       "       [0.4934 ],\n",
       "       [0.088  ],\n",
       "       [0.1708 ],\n",
       "       [0.705  ],\n",
       "       [0.659  ],\n",
       "       [0.198  ],\n",
       "       [0.4277 ],\n",
       "       [0.2793 ],\n",
       "       [0.5938 ],\n",
       "       [0.089  ],\n",
       "       [0.3865 ],\n",
       "       [0.5957 ],\n",
       "       [0.6504 ],\n",
       "       [0.1354 ],\n",
       "       [0.1765 ],\n",
       "       [0.633  ],\n",
       "       [0.3328 ],\n",
       "       [0.08826],\n",
       "       [0.3054 ],\n",
       "       [0.5815 ],\n",
       "       [0.1056 ],\n",
       "       [0.1765 ],\n",
       "       [0.634  ],\n",
       "       [0.639  ],\n",
       "       [0.2328 ],\n",
       "       [0.09906],\n",
       "       [0.1617 ],\n",
       "       [0.224  ],\n",
       "       [0.1827 ],\n",
       "       [0.373  ],\n",
       "       [0.5317 ],\n",
       "       [0.5337 ],\n",
       "       [0.665  ],\n",
       "       [0.0861 ],\n",
       "       [0.0922 ],\n",
       "       [0.198  ],\n",
       "       [0.2064 ],\n",
       "       [0.2969 ],\n",
       "       [0.6694 ],\n",
       "       [0.4978 ],\n",
       "       [0.678  ],\n",
       "       [0.0889 ],\n",
       "       [0.1339 ],\n",
       "       [0.1411 ],\n",
       "       [0.2053 ],\n",
       "       [0.2803 ],\n",
       "       [0.708  ],\n",
       "       [0.4404 ],\n",
       "       [0.3647 ],\n",
       "       [0.296  ],\n",
       "       [0.4854 ],\n",
       "       [0.5894 ],\n",
       "       [0.3362 ],\n",
       "       [0.1354 ],\n",
       "       [0.5244 ],\n",
       "       [0.3298 ],\n",
       "       [0.555  ],\n",
       "       [0.254  ],\n",
       "       [0.11334],\n",
       "       [0.2644 ],\n",
       "       [0.4663 ],\n",
       "       [0.4277 ],\n",
       "       [0.359  ],\n",
       "       [0.5894 ],\n",
       "       [0.4849 ],\n",
       "       [0.2026 ],\n",
       "       [0.4602 ],\n",
       "       [0.3232 ],\n",
       "       [0.2573 ],\n",
       "       [0.6694 ],\n",
       "       [0.36   ],\n",
       "       [0.659  ],\n",
       "       [0.1222 ],\n",
       "       [0.0946 ],\n",
       "       [0.1641 ],\n",
       "       [0.1411 ],\n",
       "       [0.344  ],\n",
       "       [0.6504 ],\n",
       "       [0.3474 ],\n",
       "       [0.2074 ],\n",
       "       [0.5317 ],\n",
       "       [0.1305 ],\n",
       "       [0.555  ],\n",
       "       [0.542  ],\n",
       "       [0.6123 ],\n",
       "       [0.11383],\n",
       "       [0.3843 ],\n",
       "       [0.3777 ],\n",
       "       [0.3262 ],\n",
       "       [0.1752 ],\n",
       "       [0.2031 ],\n",
       "       [0.3384 ],\n",
       "       [0.4614 ],\n",
       "       [0.5166 ],\n",
       "       [0.2446 ],\n",
       "       [0.4663 ],\n",
       "       [0.299  ],\n",
       "       [0.1398 ],\n",
       "       [0.1663 ],\n",
       "       [0.1866 ],\n",
       "       [0.6265 ],\n",
       "       [0.3616 ],\n",
       "       [0.4873 ],\n",
       "       [0.593  ],\n",
       "       [0.528  ],\n",
       "       [0.5273 ],\n",
       "       [0.562  ],\n",
       "       [0.3352 ],\n",
       "       [0.3262 ],\n",
       "       [0.3354 ],\n",
       "       [0.58   ],\n",
       "       [0.1512 ],\n",
       "       [0.148  ],\n",
       "       [0.1628 ],\n",
       "       [0.3645 ],\n",
       "       [0.6265 ],\n",
       "       [0.32   ],\n",
       "       [0.5327 ],\n",
       "       [0.2007 ],\n",
       "       [0.24   ],\n",
       "       [0.346  ],\n",
       "       [0.0571 ],\n",
       "       [0.0767 ],\n",
       "       [0.2491 ],\n",
       "       [0.1339 ],\n",
       "       [0.3616 ],\n",
       "       [0.6934 ],\n",
       "       [0.7104 ],\n",
       "       [0.4265 ],\n",
       "       [0.1148 ],\n",
       "       [0.1411 ],\n",
       "       [0.555  ],\n",
       "       [0.359  ],\n",
       "       [0.603  ],\n",
       "       [0.5605 ],\n",
       "       [0.2854 ],\n",
       "       [0.3245 ],\n",
       "       [0.0808 ],\n",
       "       [0.082  ],\n",
       "       [0.3347 ],\n",
       "       [0.4507 ],\n",
       "       [0.209  ],\n",
       "       [0.2178 ],\n",
       "       [0.6733 ],\n",
       "       [0.6855 ],\n",
       "       [0.6753 ],\n",
       "       [0.0922 ],\n",
       "       [0.08685],\n",
       "       [0.4724 ],\n",
       "       [0.6655 ],\n",
       "       [0.3843 ],\n",
       "       [0.1967 ],\n",
       "       [0.2064 ],\n",
       "       [0.1148 ],\n",
       "       [0.1161 ],\n",
       "       [0.1599 ],\n",
       "       [0.2004 ],\n",
       "       [0.4849 ],\n",
       "       [0.447  ],\n",
       "       [0.591  ],\n",
       "       [0.601  ],\n",
       "       [0.05252],\n",
       "       [0.0667 ],\n",
       "       [0.2074 ],\n",
       "       [0.3813 ],\n",
       "       [0.723  ],\n",
       "       [0.633  ],\n",
       "       [0.1321 ],\n",
       "       [0.12213],\n",
       "       [0.1113 ],\n",
       "       [0.4634 ],\n",
       "       [0.4695 ],\n",
       "       [0.3245 ],\n",
       "       [0.5815 ],\n",
       "       [0.492  ],\n",
       "       [0.268  ],\n",
       "       [0.3374 ],\n",
       "       [0.1652 ],\n",
       "       [0.1708 ],\n",
       "       [0.2158 ],\n",
       "       [0.2542 ],\n",
       "       [0.4758 ],\n",
       "       [0.7    ],\n",
       "       [0.5557 ],\n",
       "       [0.606  ],\n",
       "       [0.109  ],\n",
       "       [0.1222 ],\n",
       "       [0.1376 ],\n",
       "       [0.138  ],\n",
       "       [0.3896 ],\n",
       "       [0.7026 ],\n",
       "       [0.4277 ],\n",
       "       [0.591  ],\n",
       "       [0.1371 ],\n",
       "       [0.1477 ],\n",
       "       [0.3384 ],\n",
       "       [0.4438 ],\n",
       "       [0.1487 ],\n",
       "       [0.596  ],\n",
       "       [0.505  ],\n",
       "       [0.6055 ],\n",
       "       [0.101  ],\n",
       "       [0.1124 ],\n",
       "       [0.2017 ],\n",
       "       [0.2161 ],\n",
       "       [0.288  ],\n",
       "       [0.3672 ],\n",
       "       [0.6606 ],\n",
       "       [0.6123 ],\n",
       "       [0.109  ],\n",
       "       [0.1113 ],\n",
       "       [0.1371 ],\n",
       "       [0.2328 ],\n",
       "       [0.2542 ],\n",
       "       [0.485  ],\n",
       "       [0.5166 ],\n",
       "       [0.7534 ],\n",
       "       [0.0906 ],\n",
       "       [0.6885 ],\n",
       "       [0.6553 ],\n",
       "       [0.3308 ],\n",
       "       [0.4277 ],\n",
       "       [0.1105 ],\n",
       "       [0.2253 ],\n",
       "       [0.092  ],\n",
       "       [0.06537],\n",
       "       [0.1487 ],\n",
       "       [0.1641 ],\n",
       "       [0.5723 ],\n",
       "       [0.634  ],\n",
       "       [0.2067 ],\n",
       "       [0.7534 ],\n",
       "       [0.3374 ],\n",
       "       [0.11707],\n",
       "       [0.1534 ],\n",
       "       [0.698  ],\n",
       "       [0.313  ],\n",
       "       [0.4497 ],\n",
       "       [0.2004 ],\n",
       "       [0.381  ],\n",
       "       [0.431  ],\n",
       "       [0.05273],\n",
       "       [0.07776],\n",
       "       [0.4265 ],\n",
       "       [0.1396 ],\n",
       "       [0.708  ],\n",
       "       [0.1384 ],\n",
       "       [0.313  ],\n",
       "       [0.599  ],\n",
       "       [0.4531 ],\n",
       "       [0.464  ],\n",
       "       [0.1863 ],\n",
       "       [0.322  ],\n",
       "       [0.1602 ],\n",
       "       [0.2303 ],\n",
       "       [0.4355 ],\n",
       "       [0.3376 ],\n",
       "       [0.12134],\n",
       "       [0.2134 ],\n",
       "       [0.3127 ],\n",
       "       [0.1255 ],\n",
       "       [0.363  ],\n",
       "       [0.5317 ],\n",
       "       [0.596  ],\n",
       "       [0.4148 ],\n",
       "       [0.0897 ],\n",
       "       [0.12006],\n",
       "       [0.3647 ],\n",
       "       [0.254  ],\n",
       "       [0.2084 ],\n",
       "       [0.1599 ],\n",
       "       [0.431  ],\n",
       "       [0.6553 ],\n",
       "       [0.1115 ],\n",
       "       [0.11334],\n",
       "       [0.2505 ],\n",
       "       [0.2568 ],\n",
       "       [0.5605 ],\n",
       "       [0.4695 ],\n",
       "       [0.505  ],\n",
       "       [0.4148 ],\n",
       "       [0.1451 ],\n",
       "       [0.157  ],\n",
       "       [0.288  ],\n",
       "       [0.3804 ],\n",
       "       [0.314  ],\n",
       "       [0.4355 ],\n",
       "       [0.4724 ],\n",
       "       [0.542  ],\n",
       "       [0.1076 ],\n",
       "       [0.2303 ],\n",
       "       [0.3147 ],\n",
       "       [0.562  ],\n",
       "       [0.513  ],\n",
       "       [0.555  ],\n",
       "       [0.6035 ],\n",
       "       [0.3672 ],\n",
       "       [0.2754 ],\n",
       "       [0.2161 ],\n",
       "       [0.388  ],\n",
       "       [0.06122],\n",
       "       [0.722  ],\n",
       "       [0.148  ],\n",
       "       [0.634  ],\n",
       "       [0.1305 ],\n",
       "       [0.1255 ],\n",
       "       [0.1342 ],\n",
       "       [0.2568 ],\n",
       "       [0.4634 ],\n",
       "       [0.6055 ],\n",
       "       [0.4663 ],\n",
       "       [0.485  ],\n",
       "       [0.5723 ],\n",
       "       [0.12213],\n",
       "       [0.2505 ],\n",
       "       [0.492  ],\n",
       "       [0.5317 ],\n",
       "       [0.2961 ],\n",
       "       [0.3328 ],\n",
       "       [0.5405 ],\n",
       "       [0.3362 ],\n",
       "       [0.157  ],\n",
       "       [0.1602 ],\n",
       "       [0.4395 ],\n",
       "       [0.1887 ],\n",
       "       [0.3347 ],\n",
       "       [0.3127 ],\n",
       "       [0.665  ],\n",
       "       [0.8594 ],\n",
       "       [0.1411 ],\n",
       "       [0.1597 ],\n",
       "       [0.1637 ],\n",
       "       [0.1866 ],\n",
       "       [0.5327 ],\n",
       "       [0.591  ],\n",
       "       [0.5225 ],\n",
       "       [0.4497 ],\n",
       "       [0.1214 ],\n",
       "       [0.1752 ],\n",
       "       [0.606  ],\n",
       "       [0.36   ],\n",
       "       [0.591  ],\n",
       "       [0.5186 ],\n",
       "       [0.5938 ],\n",
       "       [0.4248 ],\n",
       "       [0.1384 ],\n",
       "       [0.1407 ],\n",
       "       [0.1628 ],\n",
       "       [0.593  ],\n",
       "       [0.2362 ],\n",
       "       [0.694  ],\n",
       "       [0.7104 ],\n",
       "       [0.2747 ],\n",
       "       [0.099  ],\n",
       "       [0.10803],\n",
       "       [0.303  ],\n",
       "       [0.2179 ],\n",
       "       [0.1663 ],\n",
       "       [0.692  ],\n",
       "       [0.6855 ],\n",
       "       [0.2362 ],\n",
       "       [0.2158 ],\n",
       "       [0.2186 ],\n",
       "       [0.2255 ],\n",
       "       [0.299  ],\n",
       "       [0.4185 ],\n",
       "       [0.3362 ],\n",
       "       [0.3347 ],\n",
       "       [0.4192 ],\n",
       "       [0.09686],\n",
       "       [0.11316],\n",
       "       [0.2179 ],\n",
       "       [0.5273 ],\n",
       "       [0.674  ],\n",
       "       [0.6733 ],\n",
       "       [0.3147 ],\n",
       "       [0.2754 ],\n",
       "       [0.1887 ],\n",
       "       [0.1973 ],\n",
       "       [0.224  ],\n",
       "       [0.2357 ],\n",
       "       [0.32   ],\n",
       "       [0.322  ],\n",
       "       [0.4824 ],\n",
       "       [0.4866 ],\n",
       "       [0.03732],\n",
       "       [0.0956 ],\n",
       "       [0.06946],\n",
       "       [0.1398 ],\n",
       "       [0.2803 ],\n",
       "       [0.378  ],\n",
       "       [0.6934 ],\n",
       "       [0.7046 ],\n",
       "       [0.0713 ],\n",
       "       [0.10767],\n",
       "       [0.2253 ],\n",
       "       [0.0946 ],\n",
       "       [0.296  ],\n",
       "       [0.3865 ],\n",
       "       [0.5063 ],\n",
       "       [0.723  ],\n",
       "       [0.1519 ],\n",
       "       [0.1556 ],\n",
       "       [0.1863 ],\n",
       "       [0.4395 ],\n",
       "       [0.24   ],\n",
       "       [0.6074 ],\n",
       "       [0.4934 ],\n",
       "       [0.3452 ],\n",
       "       [0.0958 ],\n",
       "       [0.12366],\n",
       "       [0.2067 ],\n",
       "       [0.2109 ],\n",
       "       [0.298  ],\n",
       "       [0.5767 ],\n",
       "       [0.5557 ],\n",
       "       [0.4614 ],\n",
       "       [0.1637 ],\n",
       "       [0.2394 ],\n",
       "       [0.381  ],\n",
       "       [0.528  ],\n",
       "       [0.4404 ],\n",
       "       [0.5073 ],\n",
       "       [0.599  ],\n",
       "       [0.378  ],\n",
       "       [0.1407 ],\n",
       "       [0.1556 ],\n",
       "       [0.2798 ],\n",
       "       [0.3347 ],\n",
       "       [0.581  ],\n",
       "       [0.4475 ],\n",
       "       [0.4824 ],\n",
       "       [0.5293 ],\n",
       "       [0.1076 ],\n",
       "       [0.303  ],\n",
       "       [0.286  ],\n",
       "       [0.1519 ],\n",
       "       [0.4507 ],\n",
       "       [0.58   ],\n",
       "       [0.5293 ],\n",
       "       [0.4866 ],\n",
       "       [0.04907],\n",
       "       [0.05414],\n",
       "       [0.1217 ],\n",
       "       [0.7026 ],\n",
       "       [0.4248 ],\n",
       "       [0.2573 ],\n",
       "       [0.705  ],\n",
       "       [0.2109 ],\n",
       "       [0.4758 ],\n",
       "       [0.2031 ],\n",
       "       [0.6396 ],\n",
       "       [0.4602 ],\n",
       "       [0.306  ],\n",
       "       [0.5186 ],\n",
       "       [0.2793 ],\n",
       "       [0.3362 ],\n",
       "       [0.1011 ],\n",
       "       [0.08435],\n",
       "       [0.2017 ],\n",
       "       [0.3223 ],\n",
       "       [0.314  ],\n",
       "       [0.2026 ],\n",
       "       [0.442  ],\n",
       "       [0.678  ],\n",
       "       [0.0966 ],\n",
       "       [0.2084 ],\n",
       "       [0.28   ],\n",
       "       [0.2644 ],\n",
       "       [0.1157 ],\n",
       "       [0.3125 ],\n",
       "       [0.6885 ],\n",
       "       [0.4854 ],\n",
       "       [0.3223 ],\n",
       "       [0.1533 ],\n",
       "       [0.1255 ],\n",
       "       [0.3232 ],\n",
       "       [0.1451 ],\n",
       "       [0.4185 ],\n",
       "       [0.4978 ],\n",
       "       [0.6396 ],\n",
       "       [0.1008 ],\n",
       "       [0.17   ],\n",
       "       [0.2053 ],\n",
       "       [0.4663 ],\n",
       "       [0.5312 ],\n",
       "       [0.3308 ],\n",
       "       [0.601  ],\n",
       "       [0.443  ],\n",
       "       [0.674  ],\n",
       "       [0.0783 ],\n",
       "       [0.209  ],\n",
       "       [0.3354 ],\n",
       "       [0.11383],\n",
       "       [0.513  ],\n",
       "       [0.3867 ],\n",
       "       [0.722  ],\n",
       "       [0.09106],\n",
       "       [0.104  ],\n",
       "       [0.2491 ],\n",
       "       [0.694  ],\n",
       "       [0.4475 ],\n",
       "       [0.4873 ],\n",
       "       [0.2178 ],\n",
       "       [0.2007 ],\n",
       "       [0.07245],\n",
       "       [0.11316],\n",
       "       [0.6035 ],\n",
       "       [0.1967 ],\n",
       "       [0.3867 ],\n",
       "       [0.634  ],\n",
       "       [0.1512 ],\n",
       "       [0.3777 ],\n",
       "       [0.1533 ],\n",
       "       [0.1973 ],\n",
       "       [0.2186 ],\n",
       "       [0.3376 ],\n",
       "       [0.3452 ],\n",
       "       [0.8594 ],\n",
       "       [0.3804 ],\n",
       "       [0.373  ],\n",
       "       [0.1617 ],\n",
       "       [0.17   ],\n",
       "       [0.175  ],\n",
       "       [0.4177 ],\n",
       "       [0.4233 ],\n",
       "       [0.608  ],\n",
       "       [0.2927 ],\n",
       "       [0.524  ]], dtype=float16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(dataset.train[0][0].edata['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55000/55000 [00:24<00:00, 2283.29it/s]\n",
      "100%|██████████| 5000/5000 [00:03<00:00, 1553.01it/s]\n",
      "100%|██████████| 10000/10000 [00:03<00:00, 2563.79it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "node_list = []\n",
    "labels = []\n",
    "edges = []\n",
    "edge_feats = []\n",
    "node_feats = []\n",
    "num_nodes = 0\n",
    "for g, label in tqdm(dataset.train):\n",
    "    node_list.append(np.arange(g.num_nodes()) + num_nodes)  # All the nodes are considered in a single graph\n",
    "    \n",
    "    labels.append(label)\n",
    "    edges.append(np.stack([g.edges()[0],g.edges()[1]]).T + num_nodes)\n",
    "    edge_feats.append(np.array(g.edata['feat']))\n",
    "    node_feats.append(np.array(g.ndata['feat']))\n",
    "    num_nodes += g.num_nodes()\n",
    "for g, label in tqdm(dataset.val):\n",
    "    node_list.append(np.arange(g.num_nodes()) + num_nodes)  # All the nodes are considered in a single graph\n",
    "    labels.append(label)\n",
    "    edges.append(np.stack([g.edges()[0],g.edges()[1]]).T + num_nodes)\n",
    "    edge_feats.append(np.array(g.edata['feat']))\n",
    "    node_feats.append(np.array(g.ndata['feat']))\n",
    "    num_nodes += g.num_nodes()\n",
    "for g, label in tqdm(dataset.test):\n",
    "    node_list.append(np.arange(g.num_nodes()) + num_nodes)  # All the nodes are considered in a single graph\n",
    "\n",
    "\n",
    "    labels.append(label)\n",
    "    edges.append(np.stack([g.edges()[0],g.edges()[1]]).T + num_nodes)\n",
    "    edge_feats.append(np.array(g.edata['feat']))\n",
    "    node_feats.append(np.array(g.ndata['feat']))\n",
    "    num_nodes += g.num_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_edges = np.concatenate(edges, axis=0)\n",
    "_labels = np.stack(labels).squeeze()\n",
    "_edge_feats = np.concatenate(edge_feats)\n",
    "_node_feats = np.concatenate(node_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "_node_list = sparse.lil_matrix((70000, num_nodes))\n",
    "\n",
    "for i, indices in enumerate(node_list):\n",
    "    _node_list[i, indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"node_feats\": _node_feats,\n",
    "    \"graph_class\": _labels,\n",
    "    \"edge\": _edges,\n",
    "    \"edge_feats\": _edge_feats,\n",
    "    \"node_list\": _node_list.tocsr(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save all dense arrays to mnist.npz, including ['node_feats', 'graph_class', 'edge', 'edge_feats']\n",
      "Save sparse matrix node_list to mnist_node_list.sparse.npz\n"
     ]
    }
   ],
   "source": [
    "from gli.utils import save_data\n",
    "save_data(\"mnist\", **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GLI",
   "language": "python",
   "name": "gli"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
