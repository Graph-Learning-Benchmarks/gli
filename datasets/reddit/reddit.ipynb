{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data loading.\n",
      "  NumNodes: 232965\n",
      "  NumEdges: 114615892\n",
      "  NumFeats: 602\n",
      "  NumClasses: 41\n",
      "  NumTrainingSamples: 153431\n",
      "  NumValidationSamples: 23831\n",
      "  NumTestSamples: 55703\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "from dgl.data import RedditDataset\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "data = RedditDataset(verbose=True)\n",
    "graph = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node features\n",
    "node_feats = graph.ndata[\"feat\"].numpy()\n",
    "# node labels\n",
    "node_class = graph.ndata[\"label\"].numpy()\n",
    "# edge list\n",
    "edge = torch.stack(graph.edges()).numpy().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gli.io import save_graph, Attribute\n",
    "\n",
    "node_attrs = [\n",
    "    Attribute(\n",
    "        \"NodeFeature\",\n",
    "         node_feats,\n",
    "        \"Node features of Reddit dataset, incorporating pretrained GloVe CommonCrawl word embeddings.\",\n",
    "        \"float\",\n",
    "        \"Tensor\",\n",
    "    ),\n",
    "    Attribute(\n",
    "        \"NodeLabel\",\n",
    "        node_class,\n",
    "        \"Node labels of Reddit dataset, int ranged from 0 to 40.\",\n",
    "        \"int\",\n",
    "        \"Tensor\",\n",
    "    )\n",
    "]\n",
    "\n",
    "metadata = save_graph(\n",
    "    name=\"reddit\",\n",
    "    edge=edge,\n",
    "    num_nodes=graph.num_nodes(),\n",
    "    node_attrs=node_attrs,\n",
    "    description=\"Reddit dataset.\",\n",
    "    cite=\"@article{hamilton2017inductive,\\ntitle={Inductive representation learning on large graphs},\\nauthor={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},\\njournal={Advances in neural information processing systems},\\nvolume={30},\\nyear={2017}}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata.json and graph data (.npz files) is now saved in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"description\": \"Reddit dataset.\",\n",
      "  \"data\": {\n",
      "    \"Node\": {\n",
      "      \"NodeFeature\": {\n",
      "        \"description\": \"Node features of Reddit dataset, incorporating pretrained GloVe CommonCrawl word embeddings.\",\n",
      "        \"type\": \"float\",\n",
      "        \"format\": \"Tensor\",\n",
      "        \"file\": \"reddit__graph__bfb7717c1f9b72842adc4af257467122.npz\",\n",
      "        \"key\": \"Node_NodeFeature\"\n",
      "      },\n",
      "      \"NodeLabel\": {\n",
      "        \"description\": \"Node labels of Reddit dataset, int ranged from 0 to 40.\",\n",
      "        \"type\": \"int\",\n",
      "        \"format\": \"Tensor\",\n",
      "        \"file\": \"reddit__graph__bfb7717c1f9b72842adc4af257467122.npz\",\n",
      "        \"key\": \"Node_NodeLabel\"\n",
      "      }\n",
      "    },\n",
      "    \"Edge\": {\n",
      "      \"_Edge\": {\n",
      "        \"file\": \"reddit__graph__bfb7717c1f9b72842adc4af257467122.npz\",\n",
      "        \"key\": \"Edge_Edge\"\n",
      "      }\n",
      "    },\n",
      "    \"Graph\": {\n",
      "      \"_NodeList\": {\n",
      "        \"file\": \"reddit__graph__Graph_NodeList__e4f77fbbcc4906feaf9f51e8d2a6da98.sparse.npz\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"citation\": \"@article{hamilton2017inductive,\\ntitle={Inductive representation learning on large graphs},\\nauthor={Hamilton, Will and Ying, Zhitao and Leskovec, Jure},\\njournal={Advances in neural information processing systems},\\nvolume={30},\\nyear={2017}}\",\n",
      "  \"is_heterogeneous\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print metadata\n",
    "print(json.dumps(metadata, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = graph.ndata[\"train_mask\"].nonzero().squeeze().numpy()\n",
    "val_set = graph.ndata[\"val_mask\"].nonzero().squeeze().numpy()\n",
    "test_set = graph.ndata[\"test_mask\"].nonzero().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "change to int64\n"
     ]
    }
   ],
   "source": [
    "from gli.io import save_task_node_classification\n",
    "\n",
    "task_data = save_task_node_classification(\n",
    "    name=\"reddit\",\n",
    "    description=\"Node classification on Reddit dataset.\",\n",
    "    feature=[\"Node/NodeFeature\"],\n",
    "    target=\"Node/NodeLabel\",\n",
    "    num_classes=41,\n",
    "    train_set=train_set,\n",
    "    val_set=val_set,\n",
    "    test_set=test_set,\n",
    "    task_id=\"1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task data (.json and .npz files) is now saved in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"description\": \"Node classification on Reddit dataset.\",\n",
      "  \"type\": \"NodeClassification\",\n",
      "  \"feature\": [\n",
      "    \"Node/NodeFeature\"\n",
      "  ],\n",
      "  \"target\": \"Node/NodeLabel\",\n",
      "  \"num_classes\": 41,\n",
      "  \"train_set\": {\n",
      "    \"file\": \"reddit__task_node_classification_1__f966ab3b42876ca118130cd1ea52237f.npz\",\n",
      "    \"key\": \"train_set\"\n",
      "  },\n",
      "  \"val_set\": {\n",
      "    \"file\": \"reddit__task_node_classification_1__f966ab3b42876ca118130cd1ea52237f.npz\",\n",
      "    \"key\": \"val_set\"\n",
      "  },\n",
      "  \"test_set\": {\n",
      "    \"file\": \"reddit__task_node_classification_1__f966ab3b42876ca118130cd1ea52237f.npz\",\n",
      "    \"key\": \"test_set\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(task_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reddit dataset.\n",
      "Node classification on Reddit dataset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=232965, num_edges=114615892,\n",
       "      ndata_schemes={'NodeFeature': Scheme(shape=(602,), dtype=torch.float32), 'NodeLabel': Scheme(shape=(), dtype=torch.int64), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gli.dataloading import read_gli_graph, read_gli_task, combine_graph_and_task\n",
    "\n",
    "g = read_gli_graph(\"./metadata.json\")\n",
    "t = read_gli_task(\"./task_node_classification_1.json\")\n",
    "data = combine_graph_and_task(g, t)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(t.split['train_set'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gli')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d44b81210f2b1cc6f5cbf794116c1ed3b756872db4baf49235947e0f03609f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
